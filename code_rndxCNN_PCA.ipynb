{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# data analysis\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import my_ML_algo as algo\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "                   'Mega Projects': 5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- read .csv files ---\n"
     ]
    }
   ],
   "source": [
    "# Read csvs\n",
    "print(\"--- read .csv files ---\")\n",
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "# train_df = train_df.dropna()\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)\n",
    "# test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 45) (120526, 44)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature engineering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_15504\\4005420388.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  perimeter = np.asarray(df['geometry'].length)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_15504\\4005420388.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  area_values = np.asarray(df['geometry'].area)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 32 :  (296146, 12)\n",
      "PCA 16 :  (296146, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_15504\\4005420388.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  perimeter = np.asarray(df['geometry'].length)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_15504\\4005420388.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  area_values = np.asarray(df['geometry'].area)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 32 :  (120526, 12)\n",
      "PCA 16 :  (120526, 4)\n",
      "train_x.shape, train_y.shape, test_x.shape :\n",
      " (296146, 20) (296146,) (120526, 20)\n"
     ]
    }
   ],
   "source": [
    "######## Feature engineering ########\n",
    "print(\"--- Feature engineering ---\")\n",
    "\n",
    "\n",
    "def get_features(df, dataset_type):\n",
    "    dic_features = {\"names\":[],\"features\":[]}\n",
    "\n",
    "    # geometry features\n",
    "    perimeter = np.asarray(df['geometry'].length)\n",
    "    perimeter = np.expand_dims(perimeter, axis=-1)\n",
    "    dic_features[\"features\"].append(perimeter)\n",
    "    dic_features[\"names\"].append(\"perimeter\")\n",
    "\n",
    "    area_values = np.asarray(df['geometry'].area)\n",
    "    area_values = np.expand_dims(area_values, axis=-1)\n",
    "    dic_features[\"features\"].append(area_values)\n",
    "    dic_features[\"names\"].append(\"area_values\")\n",
    "\n",
    "    def get_min_length_ratio(exte):\n",
    "        x, y = exte.xy\n",
    "        lengths = [\n",
    "            np.sqrt((x[i] - x[i + 1]) ** 2 + (y[i] - y[i + 1]) ** 2) for i in range(4)\n",
    "        ]\n",
    "        return np.min(lengths)\n",
    "\n",
    "    def get_max_length_ratio(exte):\n",
    "        x, y = exte.xy\n",
    "        lengths = [\n",
    "            np.sqrt((x[i] - x[i + 1]) ** 2 + (y[i] - y[i + 1]) ** 2) for i in range(4)\n",
    "        ]\n",
    "        return np.max(lengths)\n",
    "\n",
    "    \n",
    "    min_lengths = np.asarray(df[\"geometry\"].exterior.apply(get_min_length_ratio))\n",
    "    min_lengths = np.expand_dims(min_lengths,axis=-1)\n",
    "\n",
    "    max_lengths = np.asarray(df[\"geometry\"].exterior.apply(get_max_length_ratio))\n",
    "    max_lengths = np.expand_dims(max_lengths,axis=-1)\n",
    "\n",
    "    ratio_min_max_lengths = min_lengths/max_lengths\n",
    "    dic_features[\"features\"].append(ratio_min_max_lengths)\n",
    "    dic_features[\"names\"].append(\"ratio_min_max_lengths\")\n",
    "\n",
    "    # dic_features[\"features\"].append(min_lengths)\n",
    "    # dic_features[\"names\"].append(\"min_lengths\")\n",
    "\n",
    "    # dic_features[\"features\"].append(max_lengths)\n",
    "    # dic_features[\"names\"].append(\"max_lengths\")\n",
    "\n",
    "    \n",
    "    # geography features\n",
    "    le_urban_type = LabelEncoder()\n",
    "    urban_type = np.asarray(df[\"urban_type\"])\n",
    "    le_urban_type.fit(urban_type)\n",
    "    # print(\"possible urban_type list :\", list(le_urban_type.classes_))\n",
    "    urban_type = le_urban_type.transform(urban_type)\n",
    "    urban_type = np.expand_dims(urban_type, axis=-1)\n",
    "    dic_features[\"features\"].append(urban_type)\n",
    "    dic_features[\"names\"].append(\"urban_type\")\n",
    "\n",
    "    # le_geography_type = LabelEncoder()\n",
    "    # geography_type = np.asarray(df[\"geography_type\"])\n",
    "    # le_geography_type.fit(geography_type)\n",
    "    # # print(\"possible geography_type list :\", list(le_geography_type.classes_))\n",
    "    # geography_type = le_geography_type.transform(geography_type)\n",
    "    # geography_type = np.expand_dims(geography_type, axis=-1)\n",
    "    # features.append(geography_type)\n",
    "\n",
    "    # add sequence features\n",
    "\n",
    "    # kept_columns_dense_32 = np.arange(32)\n",
    "    # model_dense_32_output = np.load(f\"save/outputs/model_dense_32_{dataset_type}_output_5000_train_steps_no_val.npz\")[\"arr_0\"][:,kept_columns_dense_32]\n",
    "    # dic_features[\"features\"].append(model_dense_32_output)\n",
    "    # dic_features[\"names\"]+=[f\"model_dense_32_output_{j}\" for j in kept_columns_dense_32]\n",
    "\n",
    "    # kept_columns_dense_16 = np.arange(16)\n",
    "    # model_dense_16_output = np.load(f\"save/outputs/model_dense_16_{dataset_type}_output_5000_train_steps_no_val.npz\")[\"arr_0\"]\n",
    "    # model_dense_16_output = model_dense_16_output[:,kept_columns_dense_16]\n",
    "    # dic_features[\"features\"].append(model_dense_16_output)\n",
    "    # dic_features[\"names\"]+=[f\"model_dense_16_output_{j}\" for j in kept_columns_dense_16]\n",
    "\n",
    "    \n",
    "    # PCA on sequence features :\n",
    "\n",
    "    model_dense_32_output = np.load(f\"save/outputs/model_dense_32_{dataset_type}_output_5000_train_steps_no_val.npz\")[\"arr_0\"]\n",
    "    \n",
    "    PCA_model_dense_32_output = algo.apply_PCA(model_dense_32_output)\n",
    "    print(\"PCA 32 : \",PCA_model_dense_32_output.shape)\n",
    "    dic_features[\"features\"].append(PCA_model_dense_32_output)\n",
    "    dic_features[\"names\"]+=[f\"PCA_model_dense_32_output_{j}\" for j in np.arange(PCA_model_dense_32_output.shape[-1])]\n",
    "\n",
    "\n",
    "    model_dense_16_output = np.load(f\"save/outputs/model_dense_16_{dataset_type}_output_5000_train_steps_no_val.npz\")[\"arr_0\"]\n",
    "    PCA_model_dense_16_output = algo.apply_PCA(model_dense_16_output)\n",
    "    print(\"PCA 16 : \",PCA_model_dense_16_output.shape)\n",
    "    dic_features[\"features\"].append(PCA_model_dense_16_output)\n",
    "    dic_features[\"names\"]+=[f\"PCA_model_dense_16_output_{j}\" for j in np.arange(PCA_model_dense_16_output.shape[-1])]\n",
    "\n",
    "\n",
    "    # for feat in features:\n",
    "    #     print(feat.shape)\n",
    "\n",
    "    res = np.concatenate(dic_features[\"features\"], axis=-1)\n",
    "\n",
    "    return res,dic_features\n",
    "\n",
    "\n",
    "train_x,train_dic_features = get_features(train_df,dataset_type=\"train\")\n",
    "train_y = train_df['change_type'].apply(lambda x: change_type_map[x])\n",
    "\n",
    "index_x = test_df[\"index\"]\n",
    "test_x,_ = get_features(test_df,dataset_type=\"test\")\n",
    "\n",
    "print(\"train_x.shape, train_y.shape, test_x.shape :\\n\",\n",
    "      train_x.shape, train_y.shape, test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   22.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training set : 0.9999972175022197\n",
      "prediction on test set shape : (296146,)\n",
      "perimeter importance: 0.06397796689902732\n",
      "area_values importance: 0.002266520493634809\n",
      "ratio_min_max_lengths importance: 0.03535079426427134\n",
      "urban_type importance: 0.010504239208602656\n",
      "PCA_model_dense_32_output_0 importance: 0.03984066441008055\n",
      "PCA_model_dense_32_output_1 importance: 0.05516322053075325\n",
      "PCA_model_dense_32_output_2 importance: 0.07962050867702015\n",
      "PCA_model_dense_32_output_3 importance: 0.07461725168387211\n",
      "PCA_model_dense_32_output_4 importance: 0.06961474993311663\n",
      "PCA_model_dense_32_output_5 importance: 0.03529056999609417\n",
      "PCA_model_dense_32_output_6 importance: 0.030763754065506646\n",
      "PCA_model_dense_32_output_7 importance: 0.04528849524931228\n",
      "PCA_model_dense_32_output_8 importance: 0.030822618078052214\n",
      "PCA_model_dense_32_output_9 importance: 0.034903815462257785\n",
      "PCA_model_dense_32_output_10 importance: 0.040637224710224025\n",
      "PCA_model_dense_32_output_11 importance: 0.04999402791620833\n",
      "PCA_model_dense_16_output_0 importance: 0.06750081636007194\n",
      "PCA_model_dense_16_output_1 importance: 0.049428236454483805\n",
      "PCA_model_dense_16_output_2 importance: 0.11313622005198384\n",
      "PCA_model_dense_16_output_3 importance: 0.07127830555542623\n",
      "[42, 43, 41, 44, 48, 46, 41, 40, 44, 48, 47, 41, 44, 44, 40, 42, 44, 39, 43, 43, 42, 42, 43, 44, 42, 40, 42, 39, 47, 44, 44, 49, 44, 44, 48, 46, 45, 43, 41, 50, 47, 43, 42, 47, 43, 43, 43, 41, 38, 41, 45, 50, 47, 40, 47, 41, 43, 43, 43, 44, 42, 45, 42, 42, 43, 50, 44, 41, 41, 42, 45, 43, 47, 42, 41, 45, 40, 41, 46, 44, 44, 41, 50, 41, 42, 43, 42, 41, 41, 40, 43, 46, 37, 46, 45, 43, 48, 41, 45, 42]\n",
      "[36215, 36485, 36310, 36293, 35991, 36385, 36584, 36428, 36110, 36469, 36076, 36277, 36354, 36517, 36483, 36433, 36466, 36436, 35818, 36257, 36239, 36411, 36275, 36112, 36136, 36308, 35908, 36400, 36097, 36167, 36284, 36141, 36077, 36211, 36251, 36449, 36323, 36296, 36240, 36449, 36542, 36583, 36329, 36304, 36186, 36523, 36245, 36478, 36088, 36299, 36401, 36263, 36690, 36221, 36416, 36429, 36072, 36171, 36289, 36577, 36331, 35935, 36136, 36073, 36192, 36650, 36438, 36270, 36376, 36159, 36130, 35745, 36031, 36142, 36174, 35921, 36384, 36525, 36234, 36476, 35942, 36418, 36102, 36540, 35922, 36103, 36223, 36230, 36220, 36699, 36155, 36187, 36353, 36483, 36355, 36139, 36315, 36072, 36071, 36493]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   22.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   19.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.0s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.55824714 0.5526009  0.56332018 0.54081452 0.5521717  0.55196117\n",
      " 0.5298318  0.53255696 0.55007385 0.55472355]\n",
      "Mean 0.5486301773758064\n",
      "Std: 0.010294231286904973\n",
      "experiment saved at ./results/automatic_study_features_rndxCNN.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jtros\\CS\\cours\\Cours_de_2A\\electifML\\geoproject\\utils.py:97: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df_read.append(new_df, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "######## Training ########\n",
    "print(\"--- train ---\")\n",
    "rnd_clf = RandomForestClassifier(verbose=1,n_jobs=-1)\n",
    "\n",
    "rnd_clf.fit(train_x,train_y)\n",
    "pred_y = rnd_clf.predict(train_x)\n",
    "train_rnd_clf_f1_score = f1_score(pred_y, train_y,average='macro')\n",
    "print(\"f1_score on training set :\", train_rnd_clf_f1_score)\n",
    "print(\"prediction on test set shape :\", pred_y.shape)\n",
    "\n",
    "utils.display_feature_importances(train_dic_features,rnd_clf)\n",
    "\n",
    "print([estimator.get_depth() for estimator in rnd_clf.estimators_])\n",
    "print([estimator.get_n_leaves() for estimator in rnd_clf.estimators_])\n",
    "\n",
    "utils.plot_and_save_confusion_matrix(pred_y,train_y,\"./results/confusion_matrix_simple_rnd_clfxCNN.png\")\n",
    "\n",
    "\n",
    "\n",
    "k_fold_number = 10\n",
    "val_scores = [-1]\n",
    "if k_fold_number>1:\n",
    "    def display_scores(scores):\n",
    "        print(\"Scores:\",scores)\n",
    "        print(\"Mean\",scores.mean())\n",
    "        print(\"Std:\",scores.std())\n",
    "\n",
    "    val_scores = cross_val_score(rnd_clf,train_x,train_y,scoring=\"f1_macro\",cv=k_fold_number)\n",
    "    display_scores(val_scores)\n",
    "\n",
    "\n",
    "    utils.save_experiment_in_excel(\"./results/automatic_study_features_rndxCNN.xlsx\",rnd_clf,train_rnd_clf_f1_score,k_fold_number,val_scores,train_dic_features,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- save ---\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'my_submissions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m--- save ---\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m pred_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(pred_y, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mchange_type\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m pred_df\u001b[39m.\u001b[39;49mto_csv(\u001b[39m\"\u001b[39;49m\u001b[39m./my_submissions/my_submission_simple_rndxCNN.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, index_label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mId\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\core\\generic.py:3720\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3709\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3711\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3712\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3713\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3717\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3718\u001b[0m )\n\u001b[1;32m-> 3720\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3721\u001b[0m     path_or_buf,\n\u001b[0;32m   3722\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[0;32m   3723\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3724\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3725\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3726\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3727\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3728\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3729\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3730\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3731\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3732\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3733\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3734\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3735\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3736\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3737\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\formats\\format.py:1189\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1168\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1170\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1171\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1172\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1188\u001b[0m )\n\u001b[1;32m-> 1189\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1191\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1192\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\common.py:734\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    732\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    733\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 734\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    736\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    737\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    738\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\pandas\\io\\common.py:597\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    595\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    596\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'my_submissions'"
     ]
    }
   ],
   "source": [
    "######## Save results to submission file ########\n",
    "pred_y = rnd_clf.predict(test_x)\n",
    "print(\"--- save ---\")\n",
    "pred_df = pd.DataFrame(pred_y, columns=['change_type'])\n",
    "pred_df.to_csv(\"./my_submissions/my_submission_simple_rndxCNN.csv\", index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_geoproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a691981180cc6a4785b1489b0832c35bb891551f8845804eee4cb510316e0ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
