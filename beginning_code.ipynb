{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- read .csv files ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script can be used as skeleton code to read the challenge train and test\n",
    "geojsons, to train a trivial model, and write data to the submission file.\n",
    "\"\"\"\n",
    "# data handling\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data analysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "                   'Mega Projects': 5}\n",
    "\n",
    "# Read csvs\n",
    "print(\"--- read .csv files ---\")\n",
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines at first: 120526\n",
      "number of lines without na: 119176\n",
      "(1350,)\n",
      "(120526,)\n",
      "(120526, 44)\n"
     ]
    }
   ],
   "source": [
    "def handle_na_in_df(df):\n",
    "    print(\"number of lines at first:\",df.shape[0])\n",
    "    df_without_na = df.dropna()\n",
    "    print(\"number of lines without na:\",df_without_na.shape[0])\n",
    "    indices_without_na = np.asarray(df_without_na.index)\n",
    "    df_with_na = df[df.isna().any(axis=1)]\n",
    "    indices_with_na = np.asarray(df_with_na.index)\n",
    "\n",
    "    indices = np.concatenate([indices_without_na,indices_with_na],axis=0)\n",
    "\n",
    "    dummy_values = 2*np.ones((indices_with_na.shape[0],))\n",
    "\n",
    "    return df_without_na,indices,dummy_values\n",
    "\n",
    "df_without_na,indices,dummy_values = handle_na_in_df(test_df)\n",
    "print(dummy_values.shape)\n",
    "print(indices.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter = train_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature engineering ---\n",
      "number of lines at first: 296146\n",
      "number of lines without na: 292758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_23688\\4155196044.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  perimeter = np.asarray(df['geometry'].length)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_23688\\4155196044.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  area_values = np.asarray(df['geometry'].area)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_23688\\4155196044.py:19: RuntimeWarning: divide by zero encountered in divide\n",
      "  ratio_length_over_area = perimeter/area_values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible urban_type list : ['Dense Urban', 'Industrial', 'N,A', 'Rural', 'Sparse Urban', 'Urban Slum']\n",
      "possible geography_type list : ['Barren Land', 'Coastal', 'Dense Forest', 'Desert', 'Farms', 'Grass Land', 'Hills', 'Lakes', 'N,A', 'River', 'Snow', 'Sparse Forest']\n",
      "geography_type [[0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n",
      "['perimeter'] (292758, 1)\n",
      "['area'] (292758, 1)\n",
      "['ratio_length_over_area'] (292758, 1)\n",
      "['Dense Urban', 'Industrial', 'N,A', 'Rural', 'Sparse Urban', 'Urban Slum'] (292758, 6)\n",
      "['Barren Land', 'Coastal', 'Dense Forest', 'Desert', 'Farms', 'Grass Land', 'Hills', 'Lakes', 'N,A', 'River', 'Snow', 'Sparse Forest'] (292758, 12)\n",
      "['new_date_diff0', 'new_date_diff1', 'new_date_diff2', 'new_date_diff3', 'new_date_diff4'] (292758, 5)\n",
      "['color_mean_std_0_0_0', 'color_mean_std_1_0_0', 'color_mean_std_2_0_0', 'color_mean_std_3_0_0', 'color_mean_std_4_0_0'] (292758, 5)\n",
      "['color_mean_std_0_0_1', 'color_mean_std_1_0_1', 'color_mean_std_2_0_1', 'color_mean_std_3_0_1', 'color_mean_std_4_0_1'] (292758, 5)\n",
      "['color_mean_std_0_1_0', 'color_mean_std_1_1_0', 'color_mean_std_2_1_0', 'color_mean_std_3_1_0', 'color_mean_std_4_1_0'] (292758, 5)\n",
      "['color_mean_std_0_1_1', 'color_mean_std_1_1_1', 'color_mean_std_2_1_1', 'color_mean_std_3_1_1', 'color_mean_std_4_1_1'] (292758, 5)\n",
      "['color_mean_std_0_2_0', 'color_mean_std_1_2_0', 'color_mean_std_2_2_0', 'color_mean_std_3_2_0', 'color_mean_std_4_2_0'] (292758, 5)\n",
      "['color_mean_std_0_2_1', 'color_mean_std_1_2_1', 'color_mean_std_2_2_1', 'color_mean_std_3_2_1', 'color_mean_std_4_2_1'] (292758, 5)\n",
      "number of lines at first: 120526\n",
      "number of lines without na: 119176\n",
      "possible urban_type list : ['Dense Urban', 'Industrial', 'N,A', 'Rural', 'Sparse Urban', 'Urban Slum']\n",
      "possible geography_type list : ['Barren Land', 'Coastal', 'Dense Forest', 'Desert', 'Farms', 'Grass Land', 'Hills', 'Lakes', 'N,A', 'River', 'Snow', 'Sparse Forest']\n",
      "geography_type [[1 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]]\n",
      "train_x.shape, train_y.shape, test_x.shape :\n",
      " (292758, 56) (292758,) (119176, 56)\n"
     ]
    }
   ],
   "source": [
    "######## Feature engineering ########\n",
    "print(\"--- Feature engineering ---\")\n",
    "\n",
    "\n",
    "def get_features(df):\n",
    "    dic_features = {\"names\":[],\"features\":[]}\n",
    "\n",
    "    # geometry features\n",
    "    perimeter = np.asarray(df['geometry'].length)\n",
    "    perimeter = np.expand_dims(perimeter, axis=-1)\n",
    "    dic_features[\"features\"].append(perimeter)\n",
    "    dic_features[\"names\"].append(\"perimeter\")\n",
    "\n",
    "    area_values = np.asarray(df['geometry'].area)\n",
    "    area_values = np.expand_dims(area_values, axis=-1)\n",
    "    dic_features[\"features\"].append(area_values)\n",
    "    dic_features[\"names\"].append(\"area\")\n",
    "\n",
    "    ratio_length_over_area = perimeter/area_values\n",
    "    dic_features[\"features\"].append(ratio_length_over_area)\n",
    "    dic_features[\"names\"].append(\"ratio_length_over_area\")\n",
    "\n",
    "    diameter = \n",
    "\n",
    "\n",
    "    # geography features\n",
    "    mlb_urban_type = MultiLabelBinarizer()\n",
    "    urban_type = np.asarray(df[\"urban_type\"].apply(lambda x: x.split(\",\") if x!=\"N,A\" else [x]))\n",
    "    mlb_urban_type.fit(urban_type)\n",
    "    print(\"possible urban_type list :\", list(mlb_urban_type.classes_))\n",
    "    urban_type = mlb_urban_type.transform(urban_type)\n",
    "    dic_features[\"features\"].append(urban_type)\n",
    "    dic_features[\"names\"]+=list(mlb_urban_type.classes_)\n",
    "\n",
    "    mlb_geography_type = MultiLabelBinarizer()\n",
    "    geography_type = np.asarray(df[\"geography_type\"].apply(lambda x: x.split(\",\") if x!=\"N,A\" else [x]))\n",
    "    mlb_geography_type.fit(geography_type)\n",
    "    print(\"possible geography_type list :\", list(mlb_geography_type.classes_))\n",
    "    geography_type = mlb_geography_type.transform(geography_type)\n",
    "    print(\"geography_type\",geography_type)\n",
    "    dic_features[\"features\"].append(geography_type)\n",
    "    dic_features[\"names\"]+=list(mlb_geography_type.classes_)\n",
    "\n",
    "    # dates/images features\n",
    "    def get_sorted_date_diff_with_indices(df):\n",
    "        dates_to_add = []\n",
    "        for i in range(5):\n",
    "            date = df[f'date{i}'].apply(lambda x: int(str(x)[-4:]))\n",
    "            date = np.asarray(date)\n",
    "            date = np.expand_dims(date,axis=-1)\n",
    "            dates_to_add.append(date)\n",
    "\n",
    "        dates = np.concatenate(dates_to_add,axis=-1)\n",
    "\n",
    "        indices_dates = np.argsort(dates,axis=-1)\n",
    "        dates = np.array([dates[i,indices_dates[i,:]] for i in range(dates.shape[0])])\n",
    "\n",
    "        date_diff = dates - dates[:,-1:]@np.ones((1,5),dtype=int)\n",
    "        return date_diff, indices_dates\n",
    "\n",
    "    date_diff,indices_dates=get_sorted_date_diff_with_indices(df)\n",
    "    dic_features[\"features\"].append(date_diff)\n",
    "    dic_features[\"names\"]+=[f\"new_date_diff{i}\" for i in range(5)]\n",
    "\n",
    "    \n",
    "    trad_colors = {\"red\":0,\"blue\":1,\"green\":2}\n",
    "    def get_mean_std(df):\n",
    "        colors = list(trad_colors.keys())\n",
    "        res = np.zeros((df.shape[0],5,3,2))\n",
    "        for i in range(1,6):\n",
    "            for j_color,color in enumerate(colors):\n",
    "                res[:,i-1,j_color,0] = np.asarray(df[f\"img_{color}_mean_date{i}\"])\n",
    "                res[:,i-1,j_color,1] = np.asarray(df[f\"img_{color}_std_date{i}\"])\n",
    "                \n",
    "        return res\n",
    "\n",
    "    color_mean_std = get_mean_std(df)\n",
    "    color_mean_std = np.array([color_mean_std[i,indices_dates[i,:],:,:] for i in range(color_mean_std.shape[0])])\n",
    "\n",
    "    for i in range(color_mean_std.shape[2]):\n",
    "        for j in range(color_mean_std.shape[3]):\n",
    "            dic_features[\"features\"].append(color_mean_std[:,:,i,j])\n",
    "            dic_features[\"names\"]+=[f\"color_mean_std_{date}_{i}_{j}\" for date in range(5)]\n",
    "\n",
    "    res = np.concatenate(dic_features[\"features\"], axis=-1)\n",
    "\n",
    "    return res,dic_features\n",
    "\n",
    "def display_features(dic_features):\n",
    "    i_names = 0\n",
    "    for i_feat in range(len(dic_features[\"features\"])):\n",
    "        feat = dic_features[\"features\"][i_feat]\n",
    "        name = dic_features[\"names\"][i_names:i_names+feat.shape[-1]]\n",
    "        i_names+=feat.shape[-1]\n",
    "        print(name,feat.shape)\n",
    "\n",
    "train_df_without_na,train_indices,train_dummy_values = handle_na_in_df(train_df)\n",
    "train_x,train_dic_features= get_features(train_df_without_na)\n",
    "display_features(train_dic_features)\n",
    "train_y = train_df_without_na['change_type'].apply(lambda x: change_type_map[x])\n",
    "\n",
    "test_df_without_na,test_indices,test_dummy_values = handle_na_in_df(test_df)\n",
    "test_x,_ = get_features(test_df_without_na)\n",
    "\n",
    "print(\"train_x.shape, train_y.shape, test_x.shape :\\n\",\n",
    "      train_x.shape, train_y.shape, test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.13700987e-03  8.17460143e-07  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -6.00000000e+00 -3.00000000e+00 -2.00000000e+00 -1.00000000e+00\n",
      "  0.00000000e+00  1.25773062e+02  1.50766726e+02  1.04614233e+02\n",
      "  9.33717748e+01  9.22913469e+01  2.82699838e+01  5.57453108e+01\n",
      "  3.38131603e+01  2.98120405e+01  3.98199487e+01  1.34900701e+02\n",
      "  1.49356684e+02  1.02844339e+02  8.98273794e+01  7.95700644e+01\n",
      "  2.50080316e+01  4.27232176e+01  3.48180115e+01  2.53242936e+01\n",
      "  2.81896038e+01  1.39833243e+02  1.58964529e+02  1.00950353e+02\n",
      "  1.07291113e+02  8.87942502e+01  2.82649070e+01  4.75763832e+01\n",
      "  3.30640141e+01  2.83283680e+01  3.08642300e+01]\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   10.3s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   56.6s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  2.7min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training set : 0.8695494151963375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   35.9s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    1.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    1.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   35.8s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.23546004 0.21843254 0.24000189]\n",
      "Mean 0.23129815687661548\n",
      "Std: 0.00928440102862603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######## Training ########\n",
    "\n",
    "print(\"--- train ---\")\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500,max_depth=50,max_leaf_nodes=30000, bootstrap =True, verbose=True, n_jobs=-1)\n",
    "\n",
    "rnd_clf.fit(train_x,train_y)\n",
    "pred_y = rnd_clf.predict(train_x)\n",
    "print(\"f1_score on training set :\", f1_score(pred_y, train_y,average='macro'))\n",
    "\n",
    "# knn_clf = KNeighborsClassifier(n_neighbors=3,n_jobs=-1)\n",
    "# knn_clf.fit(train_x,train_y)\n",
    "# pred_y = knn_clf.predict(train_x)\n",
    "# print(\"f1_score on training set :\", f1_score(pred_y, train_y,average='macro'))\n",
    "\n",
    "if True:\n",
    "    def display_scores(scores):\n",
    "        print(\"Scores:\",scores)\n",
    "        print(\"Mean\",scores.mean())\n",
    "        print(\"Std:\",scores.std())\n",
    "\n",
    "    scores = cross_val_score(rnd_clf,train_x,train_y,scoring=\"f1_macro\",cv=3)\n",
    "    display_scores(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    max_leaf_nodes = trial.suggest_int(\"max_leaf_nodes\", 20000, 40000)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 100, 500)\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=n_estimators,max_leaf_nodes=max_leaf_nodes, bootstrap=True, verbose=True, n_jobs=-1)\n",
    "\n",
    "    scores = cross_val_score(rnd_clf,train_x,train_y,scoring=\"f1_macro\",cv=4)\n",
    "    return scores.mean()\n",
    "\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_params = study.best_params\n",
    "found_n_estimators = best_params[\"n_estimators\"]\n",
    "found_max_leaf_nodes = best_params[\"max_leaf_nodes\"]\n",
    "\n",
    "\n",
    "import yaml\n",
    "\n",
    "with open('hp_optim.yaml', 'w') as outfile:\n",
    "    yaml.dump({\"found_max_leaf_nodes\":found_max_leaf_nodes,\"found_n_estimators\":found_n_estimators}, outfile, default_flow_style=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(found_n_estimators,found_max_leaf_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perimeter importance: 0.10219737044553817\n",
      "area importance: 0.0036836961081480263\n",
      "Dense Urban importance: 0.006730438623292298\n",
      "Industrial importance: 0.012330979066035577\n",
      "N,A importance: 0.0020832256373468742\n",
      "Rural importance: 0.0014650795241021807\n",
      "Sparse Urban importance: 0.0030162967204882163\n",
      "Urban Slum importance: 0.000928947942529226\n",
      "Barren Land importance: 0.0034075354337502455\n",
      "Coastal importance: 0.0011276901200453322\n",
      "Dense Forest importance: 0.004083278926514398\n",
      "Desert importance: 0.001313713824412587\n",
      "Farms importance: 0.0035944781613635123\n",
      "Grass Land importance: 0.004243327447610884\n",
      "Hills importance: 0.0002834329644954271\n",
      "Lakes importance: 0.0040487115892368294\n",
      "N,A importance: 0.0009093203569023029\n",
      "River importance: 0.0033164005014205272\n",
      "Snow importance: 1.8774482851913879e-06\n",
      "Sparse Forest importance: 0.0037208802476631103\n",
      "new_date_diff0 importance: 0.01354480228506595\n",
      "new_date_diff1 importance: 0.012647771068156727\n",
      "new_date_diff2 importance: 0.011039582022025603\n",
      "new_date_diff3 importance: 0.00825901269133583\n",
      "new_date_diff4 importance: 0.0\n",
      "color_mean_std_0_0_0 importance: 0.02704787230862988\n",
      "color_mean_std_1_0_0 importance: 0.02705778314367314\n",
      "color_mean_std_2_0_0 importance: 0.026751134827326085\n",
      "color_mean_std_3_0_0 importance: 0.028252941663615957\n",
      "color_mean_std_4_0_0 importance: 0.027414609743349753\n",
      "color_mean_std_0_0_1 importance: 0.024927459458529252\n",
      "color_mean_std_1_0_1 importance: 0.02483123121123663\n",
      "color_mean_std_2_0_1 importance: 0.025796945174019828\n",
      "color_mean_std_3_0_1 importance: 0.02429122823532329\n",
      "color_mean_std_4_0_1 importance: 0.02468898714138433\n",
      "color_mean_std_0_1_0 importance: 0.03009772375596469\n",
      "color_mean_std_1_1_0 importance: 0.028846065224175554\n",
      "color_mean_std_2_1_0 importance: 0.030275994355753108\n",
      "color_mean_std_3_1_0 importance: 0.02843232634926698\n",
      "color_mean_std_4_1_0 importance: 0.029922923662282157\n",
      "color_mean_std_0_1_1 importance: 0.026219756090698824\n",
      "color_mean_std_1_1_1 importance: 0.02672874019235703\n",
      "color_mean_std_2_1_1 importance: 0.025491554326941595\n",
      "color_mean_std_3_1_1 importance: 0.02499570990581876\n",
      "color_mean_std_4_1_1 importance: 0.025156522539701517\n",
      "color_mean_std_0_2_0 importance: 0.026079918160362266\n",
      "color_mean_std_1_2_0 importance: 0.025500318520041785\n",
      "color_mean_std_2_2_0 importance: 0.026176078459756974\n",
      "color_mean_std_3_2_0 importance: 0.026292852064865348\n",
      "color_mean_std_4_2_0 importance: 0.026245371398292357\n",
      "color_mean_std_0_2_1 importance: 0.025504714242367686\n",
      "color_mean_std_1_2_1 importance: 0.025123399902767973\n",
      "color_mean_std_2_2_1 importance: 0.025193922588459868\n",
      "color_mean_std_3_2_1 importance: 0.023973883726566846\n",
      "color_mean_std_4_2_1 importance: 0.024704182470705577\n",
      "[39, 44, 46, 48, 41, 45, 44, 47, 41, 45, 46, 42, 46, 43, 43, 39, 41, 46, 43, 42, 48, 44, 50, 44, 49, 46, 44, 50, 46, 46, 44, 41, 46, 45, 44, 44, 47, 47, 47, 39, 41, 40, 44, 47, 40, 41, 42, 43, 39, 44, 45, 44, 40, 43, 42, 42, 42, 40, 43, 42, 47, 40, 42, 43, 45, 39, 41, 44, 41, 50, 43, 42, 39, 43, 44, 50, 39, 43, 44, 44, 47, 44, 45, 43, 41, 45, 47, 43, 40, 44, 40, 40, 44, 44, 44, 49, 46, 42, 41, 40, 42, 41, 47, 40, 40, 48, 46, 43, 43, 48, 41, 45, 44, 42, 39, 42, 48, 43, 45, 43, 40, 43, 44, 42, 46, 42, 49, 43, 41, 42, 43, 47, 41, 50, 44, 46, 42, 42, 42, 41, 46, 43, 40, 44, 44, 47, 43, 48, 40, 43, 42, 44, 47, 39, 42, 45, 43, 44, 44, 42, 41, 41, 43, 45, 50, 47, 45, 44, 40, 44, 40, 44, 41, 46, 45, 45, 48, 44, 45, 45, 46, 40, 47, 43, 47, 44, 40, 40, 44, 45, 42, 45, 40, 43, 40, 43, 44, 39, 46, 42, 42, 43, 44, 40, 48, 41, 47, 50, 45, 42, 44, 44, 47, 47, 48, 46, 42, 45, 42, 44, 39, 40, 46, 41, 42, 40, 44, 43, 50, 42, 38, 46, 45, 43, 46, 39, 42, 50, 45, 44, 43, 45, 42, 44, 42, 42, 42, 42, 44, 42, 42, 45, 42, 45, 45, 43, 40, 50, 40, 42, 47, 41, 48, 40, 43, 45, 44, 42, 41, 41, 44, 43, 44, 44, 44, 45, 42, 48, 42, 49, 41, 44, 46, 40, 45, 46, 45, 45, 46, 48, 48, 41, 46, 45, 46, 44, 43, 45, 44, 43, 46, 44, 39, 44, 45, 43, 41, 42, 42, 42, 43, 44, 43, 41, 50, 46, 41, 45, 43, 43, 48, 40, 43, 44, 48, 42, 42, 41, 43, 46, 45, 44, 44, 44, 43, 40, 39, 39, 43, 50, 43, 45, 42, 42, 40, 48, 44, 43, 41, 43, 46, 45, 43, 41, 40, 44, 41, 39, 43, 47, 42, 42, 45, 41, 43, 45, 40, 46, 50, 42, 45, 44, 45, 46, 45, 45, 40, 47, 45, 42, 41, 46, 43, 42, 40, 46, 40, 42, 43, 43, 48, 48, 44, 48, 42, 42, 46, 40, 42, 45, 42, 43, 43, 46, 48, 45, 41, 46, 44, 42, 41, 50, 43, 42, 42, 44, 40, 42, 41, 42, 45, 43, 40, 42, 42, 42, 45, 43, 40, 46, 46, 47, 42, 45, 45, 41, 40, 41, 39, 43, 44, 40, 45, 39, 41, 43, 43, 46, 41, 46, 43, 44, 47, 43, 48, 42, 44, 43, 42, 40, 41, 45, 40, 40, 42, 46, 47, 40, 43, 44, 44, 47, 43, 43, 43, 44, 41, 43, 50, 44, 41, 42, 44, 46, 50, 41, 41, 40, 43, 47, 45, 43, 40, 45, 43, 42, 42, 43, 47, 41]\n",
      "[30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000, 30000]\n"
     ]
    }
   ],
   "source": [
    "def display_feature_importances(dic_features,clf):\n",
    "    for i,feat_name in enumerate(dic_features[\"names\"]):\n",
    "        print(f\"{feat_name} importance:\",clf.feature_importances_[i]) \n",
    "\n",
    "display_feature_importances(train_dic_features,rnd_clf)\n",
    "print([estimator.get_depth() for estimator in rnd_clf.estimators_])\n",
    "print([estimator.get_n_leaves() for estimator in rnd_clf.estimators_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=16)]: Done 500 out of 500 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction on test set shape : (119176,)\n",
      "[3 2 2 ... 3 3 3]\n",
      "pred_y.shape after: (120526,)\n",
      "(1350,)\n",
      "(120526,)\n",
      "(120526, 1) (120526, 1)\n",
      "[0 0 0 ... 5 5 5]\n",
      "--- save ---\n"
     ]
    }
   ],
   "source": [
    "pred_y = rnd_clf.predict(test_x)\n",
    "print(\"prediction on test set shape :\", pred_y.shape)\n",
    "print(pred_y)\n",
    "\n",
    "pred_y = np.concatenate([pred_y,test_dummy_values],axis=0)\n",
    "print(\"pred_y.shape after:\",pred_y.shape)\n",
    "print(test_dummy_values.shape)\n",
    "print(test_indices.shape)\n",
    "new_test_indices = np.expand_dims(test_indices,axis=-1)\n",
    "new_pred = np.expand_dims(pred_y,axis=-1)\n",
    "print(new_pred.shape,new_test_indices.shape)\n",
    "new_pred_y = np.concatenate([new_test_indices,new_pred],axis=-1)\n",
    "new_pred_y = new_pred_y[np.argsort(new_pred[:,0],axis=0),1]\n",
    "new_pred_y = new_pred_y.astype(int)\n",
    "print(new_pred_y)\n",
    "\n",
    "\n",
    "######## Save results to submission file ########\n",
    "print(\"--- save ---\")\n",
    "pred_df = pd.DataFrame(new_pred_y, columns=['change_type'])\n",
    "pred_df.to_csv(\"my_submission.csv\", index=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_geoproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a691981180cc6a4785b1489b0832c35bb891551f8845804eee4cb510316e0ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
