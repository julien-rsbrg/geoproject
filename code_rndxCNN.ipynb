{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# data analysis\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "                   'Mega Projects': 5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- read .csv files ---\n"
     ]
    }
   ],
   "source": [
    "# Read csvs\n",
    "print(\"--- read .csv files ---\")\n",
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "# train_df = train_df.dropna()\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)\n",
    "# test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 45) (120526, 44)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature engineering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_26636\\2918220097.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  perimeter = np.asarray(df['geometry'].length)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_26636\\2918220097.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  area_values = np.asarray(df['geometry'].area)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_26636\\2918220097.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  perimeter = np.asarray(df['geometry'].length)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_26636\\2918220097.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  area_values = np.asarray(df['geometry'].area)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape, train_y.shape, test_x.shape :\n",
      " (296146, 58) (296146,) (120526, 58)\n"
     ]
    }
   ],
   "source": [
    "######## Feature engineering ########\n",
    "print(\"--- Feature engineering ---\")\n",
    "\n",
    "\n",
    "def get_features(df, dataset_type):\n",
    "    dic_features = {\"names\":[],\"features\":[]}\n",
    "\n",
    "    # geometry features\n",
    "    perimeter = np.asarray(df['geometry'].length)\n",
    "    perimeter = np.expand_dims(perimeter, axis=-1)\n",
    "    dic_features[\"features\"].append(perimeter)\n",
    "    dic_features[\"names\"].append(\"perimeter\")\n",
    "\n",
    "    area_values = np.asarray(df['geometry'].area)\n",
    "    area_values = np.expand_dims(area_values, axis=-1)\n",
    "    dic_features[\"features\"].append(area_values)\n",
    "    dic_features[\"names\"].append(\"area_values\")\n",
    "\n",
    "    def get_min_length_ratio(exte):\n",
    "        x, y = exte.xy\n",
    "        lengths = [\n",
    "            np.sqrt((x[i] - x[i + 1]) ** 2 + (y[i] - y[i + 1]) ** 2) for i in range(4)\n",
    "        ]\n",
    "        return np.min(lengths)\n",
    "\n",
    "    def get_max_length_ratio(exte):\n",
    "        x, y = exte.xy\n",
    "        lengths = [\n",
    "            np.sqrt((x[i] - x[i + 1]) ** 2 + (y[i] - y[i + 1]) ** 2) for i in range(4)\n",
    "        ]\n",
    "        return np.max(lengths)\n",
    "\n",
    "    \n",
    "    min_lengths = np.asarray(df[\"geometry\"].exterior.apply(get_min_length_ratio))\n",
    "    min_lengths = np.expand_dims(min_lengths,axis=-1)\n",
    "\n",
    "    max_lengths = np.asarray(df[\"geometry\"].exterior.apply(get_max_length_ratio))\n",
    "    max_lengths = np.expand_dims(max_lengths,axis=-1)\n",
    "\n",
    "    ratio_min_max_lengths = min_lengths/max_lengths\n",
    "    dic_features[\"features\"].append(ratio_min_max_lengths)\n",
    "    dic_features[\"names\"].append(\"ratio_min_max_lengths\")\n",
    "\n",
    "    # dic_features[\"features\"].append(min_lengths)\n",
    "    # dic_features[\"names\"].append(\"min_lengths\")\n",
    "\n",
    "    # dic_features[\"features\"].append(max_lengths)\n",
    "    # dic_features[\"names\"].append(\"max_lengths\")\n",
    "\n",
    "    \n",
    "    # geography features\n",
    "    le_urban_type = LabelEncoder()\n",
    "    urban_type = np.asarray(df[\"urban_type\"])\n",
    "    le_urban_type.fit(urban_type)\n",
    "    # print(\"possible urban_type list :\", list(le_urban_type.classes_))\n",
    "    urban_type = le_urban_type.transform(urban_type)\n",
    "    urban_type = np.expand_dims(urban_type, axis=-1)\n",
    "    dic_features[\"features\"].append(urban_type)\n",
    "    dic_features[\"names\"].append(\"urban_type\")\n",
    "\n",
    "    # le_geography_type = LabelEncoder()\n",
    "    # geography_type = np.asarray(df[\"geography_type\"])\n",
    "    # le_geography_type.fit(geography_type)\n",
    "    # # print(\"possible geography_type list :\", list(le_geography_type.classes_))\n",
    "    # geography_type = le_geography_type.transform(geography_type)\n",
    "    # geography_type = np.expand_dims(geography_type, axis=-1)\n",
    "    # features.append(geography_type)\n",
    "\n",
    "    # add sequence features\n",
    "    kept_columns_dense_32 = np.arange(32)\n",
    "    model_dense_32_output = np.load(f\"./save/model_dense_32_{dataset_type}_output_val_0f7151.npz\")[\"arr_0\"][:,kept_columns_dense_32]\n",
    "    dic_features[\"features\"].append(model_dense_32_output)\n",
    "    dic_features[\"names\"]+=[f\"model_dense_32_output_{j}\" for j in kept_columns_dense_32]\n",
    "\n",
    "    kept_columns_dense_16 = np.arange(16)\n",
    "    model_dense_16_output = np.load(f\"./save/model_dense_16_{dataset_type}_output_val_0f7151.npz\")[\"arr_0\"]\n",
    "    model_dense_16_output = model_dense_16_output[:,kept_columns_dense_16]\n",
    "    dic_features[\"features\"].append(model_dense_16_output)\n",
    "    dic_features[\"names\"]+=[f\"model_dense_16_output_{j}\" for j in kept_columns_dense_16]\n",
    "\n",
    "    kept_columns_dense_6 = np.arange(6)\n",
    "    model_dense_6_output = np.load(f\"./save/CNN_model_{dataset_type}_output_val_0f7151.npz\")[\"arr_0\"]\n",
    "    model_dense_6_output = model_dense_16_output[:,kept_columns_dense_6]\n",
    "    dic_features[\"features\"].append(model_dense_6_output)\n",
    "    dic_features[\"names\"]+=[f\"model_dense_6_output_{i}\" for i in range(model_dense_6_output.shape[-1])]\n",
    "    \n",
    "\n",
    "    # for feat in features:\n",
    "    #     print(feat.shape)\n",
    "\n",
    "    res = np.concatenate(dic_features[\"features\"], axis=-1)\n",
    "\n",
    "    return res,dic_features\n",
    "\n",
    "\n",
    "train_x,train_dic_features = get_features(train_df,dataset_type=\"train\")\n",
    "train_y = train_df['change_type'].apply(lambda x: change_type_map[x])\n",
    "\n",
    "index_x = test_df[\"index\"]\n",
    "test_x,_ = get_features(test_df,dataset_type=\"test\")\n",
    "\n",
    "print(\"train_x.shape, train_y.shape, test_x.shape :\\n\",\n",
    "      train_x.shape, train_y.shape, test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   26.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training set : 0.9999958262466073\n",
      "prediction on test set shape : (296146,)\n",
      "perimeter importance: 0.042568461604253136\n",
      "area_values importance: 0.002472180155335306\n",
      "ratio_min_max_lengths importance: 0.029104419974848073\n",
      "urban_type importance: 0.009104950318162053\n",
      "model_dense_32_output_0 importance: 0.03731982265081694\n",
      "model_dense_32_output_1 importance: 0.021667777378458646\n",
      "model_dense_32_output_2 importance: 0.024180772680058468\n",
      "model_dense_32_output_3 importance: 0.005151342002806125\n",
      "model_dense_32_output_4 importance: 0.006620448232403711\n",
      "model_dense_32_output_5 importance: 0.0016159572670685414\n",
      "model_dense_32_output_6 importance: 0.051612864565034514\n",
      "model_dense_32_output_7 importance: 0.029644530402663252\n",
      "model_dense_32_output_8 importance: 0.024773366595500473\n",
      "model_dense_32_output_9 importance: 0.02290897012661603\n",
      "model_dense_32_output_10 importance: 0.011156470125114957\n",
      "model_dense_32_output_11 importance: 0.001817187973443433\n",
      "model_dense_32_output_12 importance: 0.0021858992135542054\n",
      "model_dense_32_output_13 importance: 0.011217479920937301\n",
      "model_dense_32_output_14 importance: 0.021082456805398025\n",
      "model_dense_32_output_15 importance: 0.046900723338888645\n",
      "model_dense_32_output_16 importance: 0.026246179546843412\n",
      "model_dense_32_output_17 importance: 0.021955372068042826\n",
      "model_dense_32_output_18 importance: 0.02490091050665171\n",
      "model_dense_32_output_19 importance: 0.06515276286150518\n",
      "model_dense_32_output_20 importance: 0.0069731343308633694\n",
      "model_dense_32_output_21 importance: 0.0029909935013951792\n",
      "model_dense_32_output_22 importance: 0.001197089078776259\n",
      "model_dense_32_output_23 importance: 0.0019329227350018058\n",
      "model_dense_32_output_24 importance: 0.0026301946563781166\n",
      "model_dense_32_output_25 importance: 0.026065410908437302\n",
      "model_dense_32_output_26 importance: 0.006626673964121401\n",
      "model_dense_32_output_27 importance: 0.02646115793760608\n",
      "model_dense_32_output_28 importance: 0.0022432461824063894\n",
      "model_dense_32_output_29 importance: 0.0027247667295186052\n",
      "model_dense_32_output_30 importance: 0.0038192281329686314\n",
      "model_dense_32_output_31 importance: 0.0069214212532465935\n",
      "model_dense_16_output_0 importance: 3.7380796829422145e-05\n",
      "model_dense_16_output_1 importance: 0.04146500435623017\n",
      "model_dense_16_output_2 importance: 7.285410204630143e-05\n",
      "model_dense_16_output_3 importance: 4.7258529718464185e-06\n",
      "model_dense_16_output_4 importance: 0.05138389658077529\n",
      "model_dense_16_output_5 importance: 4.893310567235264e-05\n",
      "model_dense_16_output_6 importance: 0.03091003278958986\n",
      "model_dense_16_output_7 importance: 0.0013072859111740846\n",
      "model_dense_16_output_8 importance: 0.07441478560738495\n",
      "model_dense_16_output_9 importance: 0.007007438424583614\n",
      "model_dense_16_output_10 importance: 6.207661994504593e-05\n",
      "model_dense_16_output_11 importance: 0.0340576394493678\n",
      "model_dense_16_output_12 importance: 9.680187102228302e-07\n",
      "model_dense_16_output_13 importance: 0.008673135046504531\n",
      "model_dense_16_output_14 importance: 0.00194664543797298\n",
      "model_dense_16_output_15 importance: 0.03534253858730721\n",
      "model_dense_6_output_0 importance: 3.8440454491274156e-05\n",
      "model_dense_6_output_1 importance: 0.03285575149603262\n",
      "model_dense_6_output_2 importance: 7.082429058004729e-05\n",
      "model_dense_6_output_3 importance: 4.058404413119733e-06\n",
      "model_dense_6_output_4 importance: 0.0483019098280449\n",
      "model_dense_6_output_5 importance: 4.612911424763739e-05\n",
      "[50, 50, 55, 50, 50, 47, 42, 48, 46, 48, 46, 53, 47, 48, 51, 48, 46, 47, 51, 52, 53, 46, 47, 49, 47, 47, 55, 51, 45, 53, 50, 48, 53, 50, 62, 58, 48, 58, 52, 49, 51, 48, 45, 48, 51, 48, 52, 51, 60, 46, 46, 49, 53, 56, 49, 45, 47, 48, 51, 54, 53, 53, 48, 46, 45, 48, 46, 51, 45, 49, 44, 44, 45, 45, 52, 51, 50, 52, 48, 55, 48, 48, 48, 52, 46, 59, 49, 51, 50, 60, 46, 49, 49, 46, 49, 48, 48, 50, 45, 45]\n",
      "[42444, 42671, 42543, 42785, 42904, 42533, 42764, 43043, 42573, 42699, 42588, 42363, 42984, 42965, 42488, 42213, 42832, 42770, 42333, 42428, 42586, 42856, 42784, 42713, 42905, 42681, 42606, 42960, 43084, 42326, 42930, 42558, 42747, 42668, 42192, 42710, 42697, 42863, 42903, 42549, 42619, 42670, 42867, 42472, 43080, 43041, 42910, 42803, 42923, 42968, 42499, 42810, 42752, 42807, 42474, 42663, 42828, 42492, 42832, 42733, 42576, 42762, 42741, 42690, 42865, 42819, 42811, 42498, 42938, 42974, 42635, 42892, 42487, 42880, 42731, 42364, 42758, 42712, 43333, 42278, 42443, 42916, 42616, 42809, 42954, 42759, 42551, 42791, 42614, 42451, 42959, 42533, 42511, 42881, 42719, 42731, 42557, 42735, 42681, 42947]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m     val_scores \u001b[39m=\u001b[39m cross_val_score(rnd_clf,train_x,train_y,scoring\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf1_macro\u001b[39m\u001b[39m\"\u001b[39m,cv\u001b[39m=\u001b[39mk_fold_number)\n\u001b[0;32m     29\u001b[0m     display_scores(val_scores)\n\u001b[1;32m---> 32\u001b[0m utils\u001b[39m.\u001b[39;49msave_experiment_in_excel(\u001b[39m\"\u001b[39;49m\u001b[39m./results/automatic_study_features_rndxCNN.xlsx\u001b[39;49m\u001b[39m\"\u001b[39;49m,rnd_clf,train_rnd_clf_f1_score,k_fold_number,val_scores,train_dic_features,)\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\Cours_de_2A\\electifML\\geoproject\\utils.py:76\u001b[0m, in \u001b[0;36msave_experiment_in_excel\u001b[1;34m(dst_path, clf, train_clf_f1_score, k_fold_number, val_scores, dic_features)\u001b[0m\n\u001b[0;32m     69\u001b[0m     dic_experiment \u001b[39m=\u001b[39m {\n\u001b[0;32m     70\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtraining_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m\"\u001b[39m: train_clf_f1_score},\n\u001b[0;32m     71\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalidation_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mk_fold_number\u001b[39m\u001b[39m\"\u001b[39m: k_fold_number, \u001b[39m\"\u001b[39m\u001b[39mscores_mean\u001b[39m\u001b[39m\"\u001b[39m: val_scores\u001b[39m.\u001b[39mmean(), \u001b[39m\"\u001b[39m\u001b[39mscores_std\u001b[39m\u001b[39m\"\u001b[39m: val_scores\u001b[39m.\u001b[39mstd(), \u001b[39m\"\u001b[39m\u001b[39mscores_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmax(val_scores), \u001b[39m\"\u001b[39m\u001b[39mscores_min\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmin(val_scores)},\n\u001b[0;32m     72\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfeature_importance_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfeat_name\u001b[39m}\u001b[39;00m\u001b[39m importance\u001b[39m\u001b[39m\"\u001b[39m: clf\u001b[39m.\u001b[39mfeature_importances_[i] \u001b[39mfor\u001b[39;00m i, feat_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dic_features[\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m])}\n\u001b[0;32m     73\u001b[0m     }\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m dic_experiment\n\u001b[1;32m---> 76\u001b[0m dic_experiment \u001b[39m=\u001b[39m get_dic_experiment(\n\u001b[0;32m     77\u001b[0m     clf, train_clf_f1_score, k_fold_number, val_scores, dic_features)\n\u001b[0;32m     79\u001b[0m new_df \u001b[39m=\u001b[39m {}\n\u001b[0;32m     80\u001b[0m \u001b[39mfor\u001b[39;00m main_key, sub_dic \u001b[39min\u001b[39;00m dic_experiment\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\jtros\\CS\\cours\\Cours_de_2A\\electifML\\geoproject\\utils.py:71\u001b[0m, in \u001b[0;36msave_experiment_in_excel.<locals>.get_dic_experiment\u001b[1;34m(clf, train_clf_f1_score, k_fold_number, val_scores, dic_features)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dic_experiment\u001b[39m(clf, train_clf_f1_score, k_fold_number, val_scores, dic_features):\n\u001b[0;32m     69\u001b[0m     dic_experiment \u001b[39m=\u001b[39m {\n\u001b[0;32m     70\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtraining_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m\"\u001b[39m: train_clf_f1_score},\n\u001b[1;32m---> 71\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalidation_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mk_fold_number\u001b[39m\u001b[39m\"\u001b[39m: k_fold_number, \u001b[39m\"\u001b[39m\u001b[39mscores_mean\u001b[39m\u001b[39m\"\u001b[39m: val_scores\u001b[39m.\u001b[39;49mmean(), \u001b[39m\"\u001b[39m\u001b[39mscores_std\u001b[39m\u001b[39m\"\u001b[39m: val_scores\u001b[39m.\u001b[39mstd(), \u001b[39m\"\u001b[39m\u001b[39mscores_max\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmax(val_scores), \u001b[39m\"\u001b[39m\u001b[39mscores_min\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39mmin(val_scores)},\n\u001b[0;32m     72\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfeature_importance_info\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfeat_name\u001b[39m}\u001b[39;00m\u001b[39m importance\u001b[39m\u001b[39m\"\u001b[39m: clf\u001b[39m.\u001b[39mfeature_importances_[i] \u001b[39mfor\u001b[39;00m i, feat_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dic_features[\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m])}\n\u001b[0;32m     73\u001b[0m     }\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m dic_experiment\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "######## Training ########\n",
    "print(\"--- train ---\")\n",
    "rnd_clf = RandomForestClassifier(verbose=1,n_jobs=-1)\n",
    "\n",
    "rnd_clf.fit(train_x,train_y)\n",
    "pred_y = rnd_clf.predict(train_x)\n",
    "train_rnd_clf_f1_score = f1_score(pred_y, train_y,average='macro')\n",
    "print(\"f1_score on training set :\", train_rnd_clf_f1_score)\n",
    "print(\"prediction on test set shape :\", pred_y.shape)\n",
    "\n",
    "utils.display_feature_importances(train_dic_features,rnd_clf)\n",
    "\n",
    "print([estimator.get_depth() for estimator in rnd_clf.estimators_])\n",
    "print([estimator.get_n_leaves() for estimator in rnd_clf.estimators_])\n",
    "\n",
    "utils.plot_and_save_confusion_matrix(pred_y,train_y,\"./results/confusion_matrix_simple_rnd_clfxCNN.png\")\n",
    "\n",
    "\n",
    "\n",
    "k_fold_number = 0\n",
    "val_scores = [-1]\n",
    "if k_fold_number>1:\n",
    "    def display_scores(scores):\n",
    "        print(\"Scores:\",scores)\n",
    "        print(\"Mean\",scores.mean())\n",
    "        print(\"Std:\",scores.std())\n",
    "\n",
    "    val_scores = cross_val_score(rnd_clf,train_x,train_y,scoring=\"f1_macro\",cv=k_fold_number)\n",
    "    display_scores(val_scores)\n",
    "\n",
    "\n",
    "    utils.save_experiment_in_excel(\"./results/automatic_study_features_rndxCNN.xlsx\",rnd_clf,train_rnd_clf_f1_score,k_fold_number,val_scores,train_dic_features,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- save ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "######## Save results to submission file ########\n",
    "pred_y = rnd_clf.predict(test_x)\n",
    "print(\"--- save ---\")\n",
    "pred_df = pd.DataFrame(pred_y, columns=['change_type'])\n",
    "pred_df.to_csv(\"./my_submissions/my_submission_simple_rndxCNN.csv\", index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_geoproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a691981180cc6a4785b1489b0832c35bb891551f8845804eee4cb510316e0ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
