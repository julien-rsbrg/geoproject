{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handling\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "# data analysis\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "change_type_map = {'Demolition': 0, 'Road': 1, 'Residential': 2, 'Commercial': 3, 'Industrial': 4,\n",
    "                   'Mega Projects': 5}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- read .csv files ---\n"
     ]
    }
   ],
   "source": [
    "# Read csvs\n",
    "print(\"--- read .csv files ---\")\n",
    "train_df = gpd.read_file('train.geojson', index_col=0)\n",
    "# train_df = train_df.dropna()\n",
    "test_df = gpd.read_file('test.geojson', index_col=0)\n",
    "# test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296146, 45) (120526, 44)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Feature engineering ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_24328\\2566092071.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  perimeter = np.asarray(df['geometry'].length)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_24328\\2566092071.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  area_values = np.asarray(df['geometry'].area)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_24328\\2566092071.py:9: UserWarning: Geometry is in a geographic CRS. Results from 'length' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  perimeter = np.asarray(df['geometry'].length)\n",
      "C:\\Users\\jtros\\AppData\\Local\\Temp\\ipykernel_24328\\2566092071.py:14: UserWarning: Geometry is in a geographic CRS. Results from 'area' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  area_values = np.asarray(df['geometry'].area)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape, train_y.shape, test_x.shape :\n",
      " (296146, 36) (296146,) (120526, 36)\n"
     ]
    }
   ],
   "source": [
    "######## Feature engineering ########\n",
    "print(\"--- Feature engineering ---\")\n",
    "\n",
    "\n",
    "def get_features(df, dataset_type):\n",
    "    dic_features = {\"names\":[],\"features\":[]}\n",
    "\n",
    "    # geometry features\n",
    "    perimeter = np.asarray(df['geometry'].length)\n",
    "    perimeter = np.expand_dims(perimeter, axis=-1)\n",
    "    dic_features[\"features\"].append(perimeter)\n",
    "    dic_features[\"names\"].append(\"perimeter\")\n",
    "\n",
    "    area_values = np.asarray(df['geometry'].area)\n",
    "    area_values = np.expand_dims(area_values, axis=-1)\n",
    "    dic_features[\"features\"].append(area_values)\n",
    "    dic_features[\"names\"].append(\"area_values\")\n",
    "\n",
    "    def get_min_length_ratio(exte):\n",
    "        x, y = exte.xy\n",
    "        lengths = [\n",
    "            np.sqrt((x[i] - x[i + 1]) ** 2 + (y[i] - y[i + 1]) ** 2) for i in range(4)\n",
    "        ]\n",
    "        return np.min(lengths)\n",
    "\n",
    "    def get_max_length_ratio(exte):\n",
    "        x, y = exte.xy\n",
    "        lengths = [\n",
    "            np.sqrt((x[i] - x[i + 1]) ** 2 + (y[i] - y[i + 1]) ** 2) for i in range(4)\n",
    "        ]\n",
    "        return np.max(lengths)\n",
    "\n",
    "    \n",
    "    min_lengths = np.asarray(df[\"geometry\"].exterior.apply(get_min_length_ratio))\n",
    "    min_lengths = np.expand_dims(min_lengths,axis=-1)\n",
    "\n",
    "    max_lengths = np.asarray(df[\"geometry\"].exterior.apply(get_max_length_ratio))\n",
    "    max_lengths = np.expand_dims(max_lengths,axis=-1)\n",
    "\n",
    "    ratio_min_max_lengths = min_lengths/max_lengths\n",
    "    dic_features[\"features\"].append(ratio_min_max_lengths)\n",
    "    dic_features[\"names\"].append(\"ratio_min_max_lengths\")\n",
    "\n",
    "    # dic_features[\"features\"].append(min_lengths)\n",
    "    # dic_features[\"names\"].append(\"min_lengths\")\n",
    "\n",
    "    # dic_features[\"features\"].append(max_lengths)\n",
    "    # dic_features[\"names\"].append(\"max_lengths\")\n",
    "\n",
    "    \n",
    "    # geography features\n",
    "    le_urban_type = LabelEncoder()\n",
    "    urban_type = np.asarray(df[\"urban_type\"])\n",
    "    le_urban_type.fit(urban_type)\n",
    "    # print(\"possible urban_type list :\", list(le_urban_type.classes_))\n",
    "    urban_type = le_urban_type.transform(urban_type)\n",
    "    urban_type = np.expand_dims(urban_type, axis=-1)\n",
    "    dic_features[\"features\"].append(urban_type)\n",
    "    dic_features[\"names\"].append(\"urban_type\")\n",
    "\n",
    "    # le_geography_type = LabelEncoder()\n",
    "    # geography_type = np.asarray(df[\"geography_type\"])\n",
    "    # le_geography_type.fit(geography_type)\n",
    "    # # print(\"possible geography_type list :\", list(le_geography_type.classes_))\n",
    "    # geography_type = le_geography_type.transform(geography_type)\n",
    "    # geography_type = np.expand_dims(geography_type, axis=-1)\n",
    "    # features.append(geography_type)\n",
    "\n",
    "    # add sequence features\n",
    "    kept_columns_dense_32 = np.arange(32) # 0,6,15\n",
    "    model_dense_32_output = np.load(f\"./save/model_dense_32_{dataset_type}_output_val_0f7151.npz\")[\"arr_0\"][:,kept_columns_dense_32]\n",
    "    dic_features[\"features\"].append(model_dense_32_output)\n",
    "    dic_features[\"names\"]+=[f\"model_dense_32_output_{j}\" for j in kept_columns_dense_32]\n",
    "\n",
    "    # kept_columns_dense_16 = np.arange(16)\n",
    "    # model_dense_16_output = np.load(f\"./save/model_dense_16_{dataset_type}_output_val_0f7151.npz\")[\"arr_0\"]\n",
    "    # model_dense_16_output = model_dense_16_output[:,kept_columns_dense_16]\n",
    "    # dic_features[\"features\"].append(model_dense_16_output)\n",
    "    # dic_features[\"names\"]+=[f\"model_dense_16_output_{j}\" for j in kept_columns_dense_16]\n",
    "\n",
    "    # model_dense_6_output = np.load(f\"./save/CNN_model_{dataset_type}_output_val_0f7151.npz\")[\"arr_0\"]\n",
    "    # dic_features[\"features\"].append(model_dense_6_output)\n",
    "    # dic_features[\"names\"]+=[f\"model_dense_6_output_{i}\" for i in range(model_dense_6_output.shape[-1])]\n",
    "    \n",
    "\n",
    "    # for feat in features:\n",
    "    #     print(feat.shape)\n",
    "\n",
    "    res = np.concatenate(dic_features[\"features\"], axis=-1)\n",
    "\n",
    "    return res,dic_features\n",
    "\n",
    "\n",
    "train_x,train_dic_features = get_features(train_df,dataset_type=\"train\")\n",
    "train_y = train_df['change_type'].apply(lambda x: change_type_map[x])\n",
    "\n",
    "index_x = test_df[\"index\"]\n",
    "test_x,_ = get_features(test_df,dataset_type=\"test\")\n",
    "\n",
    "print(\"train_x.shape, train_y.shape, test_x.shape :\\n\",\n",
    "      train_x.shape, train_y.shape, test_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- train ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score on training set : 0.999992196593718\n",
      "prediction on test set shape : (296146,)\n",
      "perimeter importance: 0.05526055464408856\n",
      "area_values importance: 0.0029348011177079024\n",
      "ratio_min_max_lengths importance: 0.03841650287394325\n",
      "urban_type importance: 0.012240526669406698\n",
      "model_dense_32_output_0 importance: 0.06713321047461687\n",
      "model_dense_32_output_1 importance: 0.047203797442445676\n",
      "model_dense_32_output_2 importance: 0.05024996046352765\n",
      "model_dense_32_output_3 importance: 0.0072026297336569905\n",
      "model_dense_32_output_4 importance: 0.009515876388279271\n",
      "model_dense_32_output_5 importance: 0.0026953078869726074\n",
      "model_dense_32_output_6 importance: 0.06823878785068824\n",
      "model_dense_32_output_7 importance: 0.046594750135175785\n",
      "model_dense_32_output_8 importance: 0.04171139359928843\n",
      "model_dense_32_output_9 importance: 0.03807616872714476\n",
      "model_dense_32_output_10 importance: 0.017303020901420767\n",
      "model_dense_32_output_11 importance: 0.00235355263665645\n",
      "model_dense_32_output_12 importance: 0.0028735669355811565\n",
      "model_dense_32_output_13 importance: 0.020041797153802814\n",
      "model_dense_32_output_14 importance: 0.03385806511173673\n",
      "model_dense_32_output_15 importance: 0.07907361374875863\n",
      "model_dense_32_output_16 importance: 0.04267128719069019\n",
      "model_dense_32_output_17 importance: 0.03281689295667892\n",
      "model_dense_32_output_18 importance: 0.036712884573803395\n",
      "model_dense_32_output_19 importance: 0.09955684331079866\n",
      "model_dense_32_output_20 importance: 0.014806602188466828\n",
      "model_dense_32_output_21 importance: 0.005640418173635416\n",
      "model_dense_32_output_22 importance: 0.0016493370105966933\n",
      "model_dense_32_output_23 importance: 0.002364148094578407\n",
      "model_dense_32_output_24 importance: 0.003735029463059826\n",
      "model_dense_32_output_25 importance: 0.03903920572308848\n",
      "model_dense_32_output_26 importance: 0.010189701181181522\n",
      "model_dense_32_output_27 importance: 0.044939533986374194\n",
      "model_dense_32_output_28 importance: 0.003142979144376509\n",
      "model_dense_32_output_29 importance: 0.003648089337960947\n",
      "model_dense_32_output_30 importance: 0.005465092481675511\n",
      "model_dense_32_output_31 importance: 0.010644070688135445\n",
      "[45, 55, 53, 49, 50, 56, 47, 52, 52, 56, 47, 45, 55, 49, 59, 58, 46, 48, 48, 52, 45, 46, 48, 45, 44, 50, 47, 50, 56, 52, 49, 45, 54, 52, 58, 44, 51, 60, 57, 49, 47, 54, 49, 51, 54, 49, 49, 64, 49, 44, 50, 47, 49, 55, 50, 52, 47, 46, 46, 46, 48, 56, 50, 51, 49, 54, 54, 49, 51, 48, 54, 45, 51, 60, 51, 49, 44, 46, 50, 50, 56, 49, 55, 51, 52, 51, 52, 49, 49, 45, 54, 58, 52, 52, 48, 60, 53, 48, 48, 53]\n",
      "[43570, 43602, 43473, 43572, 43409, 43370, 43353, 43228, 43597, 43577, 43086, 43085, 43785, 43376, 43450, 43384, 43048, 43401, 43451, 43798, 43296, 43437, 43490, 43399, 43658, 43259, 43244, 43163, 43704, 42914, 43447, 43368, 43293, 43482, 43410, 43207, 43610, 43627, 43428, 43521, 43516, 43293, 43323, 43531, 43192, 43627, 43137, 43320, 43165, 43337, 43384, 43772, 43738, 43237, 43042, 43344, 43530, 43387, 43568, 43446, 43565, 43430, 43355, 43466, 43627, 43658, 43344, 43680, 43283, 43511, 43596, 43034, 43421, 43138, 43560, 43186, 43237, 43133, 43302, 43206, 43436, 43432, 43247, 43597, 43487, 43679, 43138, 43475, 43465, 43425, 43394, 43624, 43175, 43238, 43308, 43525, 43409, 43388, 43107, 43578]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.7s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   22.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   22.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   21.1s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   20.4s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [0.50965796 0.46353181 0.52191186 0.49012669 0.50921735 0.53504701\n",
      " 0.50574051 0.46860442 0.5145712  0.48680292 0.51210471 0.47613475\n",
      " 0.46198528 0.47579139 0.50365055 0.43891625 0.51299466 0.47986848\n",
      " 0.51363294 0.5108249 ]\n",
      "Mean 0.4945557827539605\n",
      "Std: 0.02397433302542609\n"
     ]
    }
   ],
   "source": [
    "######## Training ########\n",
    "print(\"--- train ---\")\n",
    "rnd_clf = RandomForestClassifier(verbose=1,n_jobs=-1)\n",
    "\n",
    "rnd_clf.fit(train_x,train_y)\n",
    "pred_y = rnd_clf.predict(train_x)\n",
    "print(\"f1_score on training set :\", f1_score(pred_y, train_y,average='macro'))\n",
    "print(\"prediction on test set shape :\", pred_y.shape)\n",
    "\n",
    "utils.display_feature_importances(train_dic_features,rnd_clf)\n",
    "\n",
    "print([estimator.get_depth() for estimator in rnd_clf.estimators_])\n",
    "print([estimator.get_n_leaves() for estimator in rnd_clf.estimators_])\n",
    "\n",
    "utils.plot_and_save_confusion_matrix(pred_y,train_y,\"./results/confusion_matrix_simple_rnd_clfxCNN.png\")\n",
    "\n",
    "\n",
    "do_cross_validation = True\n",
    "if do_cross_validation:\n",
    "    def display_scores(scores):\n",
    "        print(\"Scores:\",scores)\n",
    "        print(\"Mean\",scores.mean())\n",
    "        print(\"Std:\",scores.std())\n",
    "\n",
    "    scores = cross_val_score(rnd_clf,train_x,train_y,scoring=\"f1_macro\",cv=20)\n",
    "    display_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- save ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 100 out of 100 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "######## Save results to submission file ########\n",
    "pred_y = rnd_clf.predict(test_x)\n",
    "print(\"--- save ---\")\n",
    "pred_df = pd.DataFrame(pred_y, columns=['change_type'])\n",
    "pred_df.to_csv(\"my_submission_simple_rndxCNN.csv\", index=True, index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_geoproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a691981180cc6a4785b1489b0832c35bb891551f8845804eee4cb510316e0ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
